 ### Adam优化器结合了其他两个优化器的优点：ADAgrad和RMSprop
 - ADAgrad优化器实际上为每个参数和每个时间步骤使用不同的学习率。ADAgrad背后的原因是，不频繁的参数必须有较大的学习率，而频繁的参数必须有较小的学习率。
 - RMSprop只考虑使用一定数量的前一个梯度来修正学习速率递减的问题。
 
 ### 正则化
 训练模型的另一个重要方面是确保权重不要太大，并开始关注于一个数据点，因此会过度拟合。因此，包括对大权重的惩罚（大的定义将取决于所使用的正则化器的类型）。如Tikhonov正则化
 
 ### ashare_lstm_v1：20190206-基于RNN和LSTM的股市预测方法
  - 单股测试效果：类似MA，测试代码中，预测股价效果类似MA8
  - A股票训练的模型，用在B股票上，效果类似（可能是因为beta占大部分，todo：应该试试测试和训练时间段隔开一段）
  - 标准化方法：
  
 ### 20190211-严谨解决5中机器学习算法在预测股价的应用
  - ashare_xgboost：xgboost在A股单股测试中没有重复案例中的效果
  - lstm的