### Adam优化器结合了其他两个优化器的优点：ADAgrad和RMSprop
 - ADAgrad优化器实际上为每个参数和每个时间步骤使用不同的学习率。ADAgrad背后的原因是，不频繁的参数必须有较大的学习率，而频繁的参数必须有较小的学习率。
 - RMSprop只考虑使用一定数量的前一个梯度来修正学习速率递减的问题。
 
 ### 正则化
 训练模型的另一个重要方面是确保权重不要太大，并开始关注于一个数据点，因此会过度拟合。因此，包括对大权重的惩罚（大的定义将取决于所使用的正则化器的类型）。如Tikhonov正则化
 
 